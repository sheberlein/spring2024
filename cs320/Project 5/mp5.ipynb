import os
import pandas as pd
import geopandas as gpd
import numpy as np
from matplotlib.colors import ListedColormap
import rasterio
from rasterio.mask import mask
import matplotlib
import matplotlib.pyplot as plt
import sqlite3
# new import statements
from sklearn.linear_model import LinearRegression
import sklearn
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
#Q1
gdf = gpd.read_file("counties.geojson")
num_counties = len(gdf["NAME"])
num_counties
#Q2
fig, ax = plt.subplots()
gdf.plot(ax=ax, column="POP100", legend=True)
# feature 1
conn = sqlite3.connect("counties_tracts.db")
var = pd.read_sql("""SELECT * FROM counties""", conn)
gdf["AREALAND"] = var["AREALAND"]
train, test = train_test_split(gdf, random_state=250, test_size=0.25)
#Q3
test_counties = list(test["NAME"])
test_counties
#Q4
xcols = ["AREALAND"]
ycol = "POP100"

model = LinearRegression()
model.fit(train[xcols], train[ycol])
score = model.score(test[xcols], test[ycol])
score
#Q5
predict_df = pd.DataFrame({"AREALAND": [600*2.59*1000000]})
predicted_val = model.predict(predict_df)[0]
predicted_val
# feature 2
feature2 = pd.read_sql("""SELECT SUM(tracts.HU100) as HU100, counties.NAME FROM tracts LEFT JOIN counties ON tracts.COUNTY = counties.COUNTY GROUP BY counties.NAME""", conn)
gdf.set_index('NAME', inplace=True)
feature2.set_index('NAME', inplace=True)
feature2 = feature2.reindex(gdf.index)
gdf.reset_index(inplace=True)
feature2.reset_index(inplace=True)
gdf["HU100"] = feature2["HU100"]
train, test = train_test_split(gdf, random_state=250, test_size=0.25)
#Q6
counties_q6 = list(test["NAME"])
counties_q6
#Q7
hu_dict = {}
for county in counties_q6:
    hu_dict[county] = gdf.set_axis(gdf["NAME"]).loc[county, "HU100"]
hu_dict
#Q8
xcols1 = ["HU100"]
ycol1 = "POP100"
model1 = LinearRegression()
scores = cross_val_score(model1, train[xcols1], train[ycol1])
mean_scores = scores.mean()
mean_scores
#Q9
std_scores = scores.std()
std_scores
#Q10
model2 = LinearRegression()
model.fit(train[xcols1], train[ycol1])
return_string = "POP100 = ?1?*HU100 + ?2?"
return_string = return_string.replace("?1?", str(round(model.coef_[0], 2)))
return_string = return_string.replace("?2?", str(round(model.intercept_, 2)))
return_string
#Q11
predict_df = pd.DataFrame({"HU100": range(0, 420000, 1000)})
predict_df["POP100"] = model.predict(predict_df)
ax = predict_df.plot.line(x="HU100", y="POP100", color="r", label = "Predicted")
gdf.plot.scatter(ax=ax, x="HU100", y="POP100", label="Actual")
ax.text(gdf.set_axis(gdf["NAME"]).loc["Brown County", "HU100"], gdf.set_axis(gdf["NAME"]).loc["Brown County", "POP100"], "Brown", ha="left", va="top")

#Q12
A = np.array([
    [0,0,5,8,4],
    [1,2,4,0,3],
    [2,4,0,9,2],
    [3,5,2,1,1],
    [0,5,0,1,0]
])
B = A[A>=3]
C = B[B<=6]
num = len(list(C.astype(int)))
num
#Q13

c = np.zeros((256,3))
c[0] = [0.00000000000, 0.00000000000, 0.00000000000]
c[11] = [0.27843137255, 0.41960784314, 0.62745098039]
c[12] = [0.81960784314, 0.86666666667, 0.97647058824]
c[21] = [0.86666666667, 0.78823529412, 0.78823529412]
c[22] = [0.84705882353, 0.57647058824, 0.50980392157]
c[23] = [0.92941176471, 0.00000000000, 0.00000000000]
c[24] = [0.66666666667, 0.00000000000, 0.00000000000]
c[31] = [0.69803921569, 0.67843137255, 0.63921568628]
c[41] = [0.40784313726, 0.66666666667, 0.38823529412]
c[42] = [0.10980392157, 0.38823529412, 0.18823529412]
c[43] = [0.70980392157, 0.78823529412, 0.55686274510]
c[51] = [0.64705882353, 0.54901960784, 0.18823529412]
c[52] = [0.80000000000, 0.72941176471, 0.48627450980]
c[71] = [0.88627450980, 0.88627450980, 0.75686274510]
c[72] = [0.78823529412, 0.78823529412, 0.46666666667]
c[73] = [0.60000000000, 0.75686274510, 0.27843137255]
c[74] = [0.46666666667, 0.67843137255, 0.57647058824]
c[81] = [0.85882352941, 0.84705882353, 0.23921568628]
c[82] = [0.66666666667, 0.43921568628, 0.15686274510]
c[90] = [0.72941176471, 0.84705882353, 0.91764705882]
c[95] = [0.43921568628, 0.63921568628, 0.72941176471]
custom_cmap = ListedColormap(c)

brown = gpd.read_file("counties.geojson")
#land = rasterio.open("zip://land.zip")
land = rasterio.open("zip://../mp5/land.zip!wi.tif")
matrix, _ = mask(land, [(brown.to_crs(land.crs)).set_axis(brown["NAME"]).loc["Brown County", "geometry"]], crop=True)
matrix = matrix[0]
fig, ax = plt.subplots(figsize=(6,6))
ax.imshow(matrix, vmin=0, vmax=255, cmap=custom_cmap)
#Q14
m = matrix[matrix != 0]
percent = (m == 11).astype(int).mean()
percent
#Q15
pop_dict = {"Herbacious Wetlands Cells in County": [], "POP": []}
brown2 = (brown.to_crs(land.crs)).set_axis(brown["NAME"])
for county in brown["NAME"]:
    matrix, _ = mask(land, [brown2.loc[county, "geometry"]], crop=True)
    matrix = matrix[0]
    m1 = matrix[matrix != 0]
    length = (m1 == 95).astype(int).sum()
    pop_dict["Herbacious Wetlands Cells in County"].append(length)
    pop_dict["POP"].append(brown.set_axis(brown["NAME"]).loc[county, "POP100"])

pop_df = pd.DataFrame(pop_dict)
pop_df.plot.scatter(x="Herbacious Wetlands Cells in County", y="POP")
shape = gpd.read_file('tracts.shp')
land_use = {"open_water": 11,
            "ice_snow": 12,
            "developed_open": 21,
            "developed_low": 22,
            "developed_med": 23,
            "developed_high": 24,
            "barren": 31,
            "deciduous": 41,
            "evergreen": 42,
            "mixed_forest": 43,
            "dwarf_scrub": 51,
            "shrub_scrub": 52,
            "grassland": 71,
            "sedge": 72,
            "lichens": 73,
            "moss": 74,
            "pasture": 81,
            "crops": 82,
            "woody_wetlands": 90,
            "herbacious_wetlands": 95}
add_to_shape = {}
for key in land_use:
    add_to_shape[key] = []
shape2 = shape.to_crs(land.crs).set_axis(shape["GEOID"])
for geoid in shape["GEOID"]:
    matrix, _ = mask(land, [shape2.at[geoid, "geometry"]], crop=True)
    matrix = matrix[0]
    m1 = matrix[matrix != 0]
    for key in land_use:
        add_to_shape[key].append((m1 == land_use[key]).astype(int).sum())
for key in add_to_shape:
    shape[key] = pd.Series(add_to_shape[key])
#Q16
train, test = train_test_split(shape, random_state=300, test_size=0.20)
xcols = ["open_water", "ice_snow", "developed_open", "developed_low", "developed_med", "developed_high", "barren", "deciduous", 
         "evergreen", "mixed_forest", "dwarf_scrub", "shrub_scrub", "grassland", "sedge", "lichens", "moss", "pasture", "crops", 
         "woody_wetlands", "herbacious_wetlands"]
ycol = "POP100"
model = LinearRegression()
model.fit(train[xcols], train[ycol])
predictions = model.predict(test[xcols])
pd.Series(model.coef_, index=xcols).plot.bar(xlabel="Feature", ylabel="Coefficient of Feature")
# The graph is showing the different coefficients associated with each feature in the land use data. A bigger
# coefficient means the model is relying more on that feature than others. Negative coefficients signify that
# there is an inverse relationship with that feature. So, if there is more of a particular land type, there
# will be a lower population if the coefficient is negative.
# When building my own model, I can choose to look at features with higher coefficients to further evaluate
# their affect on population.
# MODEL 1: Using columns with a clear positive coefficient
train, test = train_test_split(shape, random_state=300, test_size=0.20)
xcols3 = ["ice_snow", "developed_open", "developed_low", "developed_med"]
ycol3 = "POP100"
model1 = LinearRegression()
mean_cross_score1 = cross_val_score(model1, train[xcols3], train[ycol3]).mean()
var1 = cross_val_score(model1, train[xcols3], train[ycol3]).var()

# MODEL 2: Using columns with a clear negative coefficient
train2, test2 = train_test_split(shape, random_state=300, test_size=0.20)
xcols4 = ["developed_high", "barren", "mixed_forest", "grassland"]
ycol4 = "POP100"
model2 = LinearRegression()
mean_cross_score2 = cross_val_score(model2, train2[xcols4], train2[ycol4]).mean()
var2 = cross_val_score(model2, train2[xcols4], train2[ycol4]).var()

print(mean_cross_score1, var1, mean_cross_score2, var2)
#Q17
# The models mentioned are in the above cell.
# I recommend that model 1 is used. For model 1, the mean cross validation score is 0.403, while the mean cross validation score
# for model 2 is 0.0847. Model 1 clearly has a much higher mean cross validation score, meaning it does better against different
# train / test splits. However, model 1 does have a higher variance than model 2. Model 1's variance is 0.002636, while model 2's
# variance is 0.001299. I still think that model 1 is better, since there is not a huge difference in the variance scores.
# Model 1 is also quite simple, taking only the columns with a positive coefficient.

# fitting the model, making predictions, and calculating explained variance score.
model1.fit(train[xcols3], train[ycol3])
predictions = model1.predict(test[xcols3])
explained_variance_score = sklearn.metrics.explained_variance_score(test[ycol3], predictions)
explained_variance_score

